num_tokens: 147
num_classes: 20
dim_model: 256
num_heads: 16
num_encoder_layers: 5
num_decoder_layers: 9
dropout_p: 0.1
feat_embedding_type: 'Conv1d'
feat_embedding_in_channels: 512  # feat_depth, the input text feature is 768,clip is 512
feat_embedding_out_channels: 1024  # n_embd
feat_embedding_kernel_size: 1
feat_embedding_padding: 0
gpt_vocab_size: 256 # the size of codebook,
gpt_block_size: 265  # 53*5 + 1
gpt_n_layer: 19
gpt_n_head: 16
gpt_n_embd: 1024
bs: 32
maxemg: 100
embedding: True
cheat: False
transformer: False
num_workers: 4
percent: 1.0
classif: False
resume: False
device: 'cuda'
checkpoint_path: 'checkpoints'
learn_rate: 0.000008
threed: False
num_epochs: 100000
name: 'basicconv2dsmalllr'
log_path: 'logs'
shifted: True
seed: 100
lr_decay: 1.0
plot: False
modelname: 'test'
l1_lw: 1.0
gradient_clip: 0.0
optim: 'adamw'
data_path_train: '../../../vondrick/mia/VIBE/trainfinalspecial.txt'
data_path_val: '../../../vondrick/mia/VIBE/valfinalspecial.txt'
